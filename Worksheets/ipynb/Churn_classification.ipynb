{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing classifiers for the Churn problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The churn problem is of great interest to companies that rely on user subscriptions: will a customer renew their contract or not? We'll load a small public churn dataset using the magic of the <a href=http://pandas.pydata.org/>Pandas package</a>. Then we'll compare the performance of a variety of classifiers.\n",
    "\n",
    "As you'll see, in certain domains like this one, it can be quite useful to model subtle interactions between variables. Personally, I find it somewhat remarkable that an ensemble of decision trees performs so much better than a linear model. It would be interesting to investigate further -- which interactions are so useful? And what do they reveal about churn? You might think about what analysis you would do to study these interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use pandas to read the csv.\n",
    "churn_df = pd.read_csv('../Data/churn.csv')\n",
    "\n",
    "# Isolate target data.\n",
    "churn_result = churn_df['Churn?']\n",
    "Y = np.where(churn_result == 'True.', 1, 0)\n",
    "\n",
    "# Remove target and sparse features.\n",
    "to_drop = ['State', 'Area Code', 'Phone', 'Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop, axis = 1)\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\", \"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
    "\n",
    "# Get the feature names.\n",
    "feature_names = np.array(churn_feat_space.columns)\n",
    "\n",
    "# Thanks pandas! We'll use numpy from here on out.\n",
    "X = churn_feat_space.as_matrix().astype(np.float)\n",
    "\n",
    "# Shuffle data.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "test_data, test_labels = X[2833:], Y[2833:]\n",
    "dev_data, dev_labels = X[2333:2833], Y[2333:2833]\n",
    "train_data, train_labels = X[:2333], Y[:2333]\n",
    "\n",
    "print \"Majority class (no churn) for dev data:\", sum(dev_labels == 0) / 500.\n",
    "print \"Majority class (no churn) for test data:\", sum(test_labels == 0) / 500.\n",
    "\n",
    "for i, name in enumerate(feature_names):\n",
    "    print i, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data to have 0 mean and 1 standard deviation. Note that the feature means and standard deviations are calculated on the training data (using fit_transform). Then we subtract these means and divide by these standard deviations to get the scaled dev and test data (using transform). Fitting the scaler on the evaluation data would be a subtle form of cheating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# This overwrites the data, so be careful!\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "dev_data = scaler.transform(dev_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "print train_data.shape\n",
    "print dev_data.shape\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a bunch of different classifiers. Experiment with different sorts of non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Gaussian NB accuracy: 0.848 (dev) 0.844 (test)\n",
      "                        (with degree 2)  accuracy: 0.796 (dev) 0.808 (test)\n",
      "\n",
      "                     Logistic Regression accuracy: 0.864 (dev) 0.838 (test)\n",
      "                        (with degree 2)  accuracy: 0.906 (dev) 0.890 (test)\n",
      "\n",
      "                      SVM, linear kernel accuracy: 0.866 (dev) 0.834 (test)\n",
      "                        (with degree 2)  accuracy: 0.916 (dev) 0.898 (test)\n",
      "\n",
      "                        SVM, poly kernel accuracy: 0.916 (dev) 0.908 (test)\n",
      "                        (with degree 2)  accuracy: 0.898 (dev) 0.884 (test)\n",
      "\n",
      "                           Decision Tree accuracy: 0.910 (dev) 0.898 (test)\n",
      "                        (with degree 2)  accuracy: 0.886 (dev) 0.900 (test)\n",
      "\n",
      "                           Random Forest accuracy: 0.952 (dev) 0.942 (test)\n",
      "                        (with degree 2)  accuracy: 0.948 (dev) 0.932 (test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    (GaussianNB(),             'Gaussian NB'),\n",
    "    (LogisticRegression(),     'Logistic Regression'),\n",
    "    (SVC(kernel='linear'),     'SVM, linear kernel'),\n",
    "    (SVC(kernel='poly'),       'SVM, poly kernel'),\n",
    "    (DecisionTreeClassifier(), 'Decision Tree'),\n",
    "    (RandomForestClassifier(n_estimators=25), 'Random Forest'),\n",
    "]\n",
    "\n",
    "# So we always get the same result.\n",
    "np.random.seed(0)\n",
    "\n",
    "for clf, name in classifiers:\n",
    "    clf.fit(train_data, train_labels)\n",
    "    print '%40s accuracy: %.3f (dev) %.3f (test)' %(name, \n",
    "                                                    clf.score(dev_data, dev_labels), \n",
    "                                                    clf.score(test_data, test_labels))\n",
    "\n",
    "    # Get all pairwise combinations of the features.\n",
    "    polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "    # Run the classifier using the polynomial feature space.\n",
    "    pipeline = Pipeline([\n",
    "        ('polynomial_features', polynomial_features),\n",
    "        (name, clf)\n",
    "    ])\n",
    "    pipeline.fit(train_data, train_labels)\n",
    "    print '%40s accuracy: %.3f (dev) %.3f (test)' %('(with degree 2) ', \n",
    "                                                    pipeline.score(dev_data, dev_labels),\n",
    "                                                    pipeline.score(test_data, test_labels))\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
