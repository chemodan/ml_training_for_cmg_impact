{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and test a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the iris data. In case you don't feel familiar with the iris varieties yet, here are some pictures. The petals are smaller and stick out above the larger, flatter sepals. In many flowers, the sepal is a greenish support below the petals, but the iris sepals are designed specifically as landing pads for bumblebees, and the bright yellow coloring on the sepal directs the bees down into the tight space where pollination happens.\n",
    "\n",
    "<img src=\"../Extra/iris.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris target names: ['setosa' 'versicolor' 'virginica']\n",
      "Iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print 'Iris target names:', iris.target_names\n",
    "print 'Iris feature names:', iris.feature_names\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:100], Y[:100]\n",
    "test_data, test_labels = X[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris feature values are real valued -- measurements in centimeters. Let's look at histograms of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAADSCAYAAAA7WjOOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15be4f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new figure and set the figsize argument so we get square-ish plots of the 4 features.\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Iterate over the features, creating a subplot with a histogram for each one.\n",
    "for feature in range(train_data.shape[1]):\n",
    "    plt.subplot(1, 4, feature+1)\n",
    "    plt.hist(train_data[:,feature], 2)\n",
    "    plt.title(iris.feature_names[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simple, let's binarize these feature values. That is, we'll treat each measurement as either \"short\" or \"long\". I'm just going to choose a threshold for each feature.  Binning usually depends on the distribution.  There are two ways to bin data: constant bin width and constant bin size. For now let's just go with the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function that applies a threshold to turn real valued iris features into 0/1 features.\n",
    "# 0 will mean \"short\" and 1 will mean \"long\".\n",
    "def binarize_iris(data, thresholds=[6.0, 3.0, 2.5, 1.0]):\n",
    "    # Initialize a new feature array with the same shape as the original data.\n",
    "    binarized_data = np.zeros(data.shape)\n",
    "\n",
    "    # Apply a threshold  to each feature.\n",
    "    for feature in range(data.shape[1]):\n",
    "        binarized_data[:,feature] = data[:,feature] > thresholds[feature]\n",
    "    return binarized_data\n",
    "\n",
    "# Create new binarized training and test data\n",
    "binarized_train_data = binarize_iris(train_data)\n",
    "binarized_test_data = binarize_iris(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Naive Bayes assumes conditional independence of features. With $Y$ the set of labels and $X$ the set of features ($y$ is a specific label and $x$ is a specific feature), Naive Bayes gives the probability of a label $y$ given input features $X$ as:\n",
    "\n",
    "$ \\displaystyle P(y|X) \\approx \n",
    "  \\frac { P(y) \\prod_{x \\in X} P(x|y) }\n",
    "        { \\sum_{y \\in Y} P(y) \\prod_{x \\in X} P(x|y) }\n",
    "$\n",
    "\n",
    "Let's estimate some of these probabilities using maximum likelihood, which is just a matter of counting and normalizing. We'll start with the prior probability of the label $P(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "[31, 33, 36]\n",
      "100\n",
      "         setosa : 0.31\n",
      "     versicolor : 0.33\n",
      "      virginica : 0.36\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for all labels to zero.\n",
    "label_counts = [0 for i in iris.target_names]\n",
    "#  print label_counts\n",
    "print iris.target_names\n",
    "\n",
    "# Iterate over labels in the training data and update counts.\n",
    "for label in train_labels:\n",
    "    label_counts[label] += 1\n",
    "print label_counts\n",
    "\n",
    "# Normalize counts to get the estimates of the probabilities of each label.\n",
    "total = sum(label_counts)\n",
    "print total\n",
    "label_probs = [1.0 * count / total for count in label_counts]\n",
    "for (prob, name) in zip(label_probs, iris.target_names):\n",
    "    print '%15s : %.2f' %(name, prob)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what zip() function does (from https://docs.python.org/2/library/functions.html#zip):             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "zipped = zip(x, y)\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have estimated the prior probabilities of each label, $P(y)$\n",
    "\n",
    "Next, let's estimate $P(X|Y)$, that is, the probability of each feature given each label: if I am a flower labeled $y (e.g., setosa)$, what is the probability that my measurements (features) will be $x$?  \n",
    "\n",
    "Remember that we can get the conditional probability from the joint distribution:\n",
    "\n",
    "$\\displaystyle P(X|Y) = \\frac{ P(X,Y) } { P(Y) } \\approx \\frac{ \\textrm{Count}(X,Y) } { \\textrm{Count}(Y) }$\n",
    "\n",
    "Let's think carefully about the size of the count matrix we need to collect. There are 3 labels $y_1$, $y_2$, and $y_3$ ($setosa$, $versicolor$, and $virginica$) and 4 features $x_0$, $x_1$, $x_2$, and $x_3$ ($petalLength$, $petalWidth$, $sepalLength$, and $sepalWidth$). Each feature has 2 possible values, 0 or 1. So there are actually $4 \\times 2 \\times 3=24$ probabilities we need to estimate: \n",
    "\n",
    "$P(x_0=0, Y=y_0)$\n",
    "\n",
    "$P(x_0=1, Y=y_0)$\n",
    "\n",
    "$P(x_1=0, Y=y_0)$\n",
    "\n",
    "$P(x_1=1, Y=y_0)$\n",
    "\n",
    "...\n",
    "\n",
    "However, we already estimated (above) the probability of each label, $P(y)$. And, we know that each feature value is either 0 or 1. So, for example,\n",
    "\n",
    "$P(x_0=0, Y=\\textrm{setosa}) + P(x_0=1, Y=\\textrm{setosa}) = P(Y=\\textrm{setosa}) \\approx 0.31$.\n",
    "\n",
    "As a result, we can just estimate probabilities for one of the feature values, say, $x_i = 0$. This requires a $4 \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature = 0 and label:\n",
      "[[ 31.  20.   7.]\n",
      " [  6.  27.  25.]\n",
      " [ 31.   0.   0.]\n",
      " [ 31.   3.   0.]]\n",
      "\n",
      "Feature = 1 and label:\n",
      "[[  0.  13.  29.]\n",
      " [ 25.   6.  11.]\n",
      " [  0.  33.  36.]\n",
      " [  0.  30.  36.]]\n",
      "\n",
      "Total count: 400.0\n",
      "Label probabilities: [ 0.31  0.33  0.36]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a matrix for joint counts of feature=0 and label.\n",
    "feature0_and_label_counts = np.zeros([len(iris.feature_names), len(iris.target_names)])\n",
    "\n",
    "# Just to check our work, let's also keep track of joint counts of feature=1 and label.\n",
    "feature1_and_label_counts = np.zeros([len(iris.feature_names), len(iris.target_names)])\n",
    "\n",
    "for i in range(binarized_train_data.shape[0]):\n",
    "    # Pick up one training example at a time: a label and a feature vector.\n",
    "    label = train_labels[i]\n",
    "    features = binarized_train_data[i]\n",
    "    \n",
    "    # Update the count matrices.\n",
    "    for feature_index, feature_value in enumerate(features):\n",
    "        feature0_and_label_counts[feature_index][label] += (feature_value == 0)\n",
    "        feature1_and_label_counts[feature_index][label] += (feature_value == 1)\n",
    "\n",
    "# Let's look at the counts.\n",
    "print 'Feature = 0 and label:\\n', feature0_and_label_counts\n",
    "print '\\nFeature = 1 and label:\\n', feature1_and_label_counts\n",
    "\n",
    "# As a sanity check, what should the total sum of all counts be?\n",
    "# We have 100 training examples, each with 4 features. So we should have counted 400 things.\n",
    "total_sum = feature0_and_label_counts.sum() + feature1_and_label_counts.sum()\n",
    "print '\\nTotal count:', total_sum\n",
    "\n",
    "# As another sanity check, the label probabilities should be equal to the normalized feature counts for each label.\n",
    "print 'Label probabilities:', (feature0_and_label_counts.sum(0) + feature1_and_label_counts.sum(0)) / total_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to normalize the joint counts to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated values of P(feature=0|label):\n",
      "[[ 1.          0.60606061  0.19444444]\n",
      " [ 0.19354839  0.81818182  0.69444444]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 1.          0.09090909  0.        ]]\n",
      "\n",
      "Check that P(feature=0|label) + P(feature=1|label) = 1\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize new matrices to hold conditional probabilities.\n",
    "feature0_given_label = np.zeros(feature0_and_label_counts.shape)\n",
    "feature1_given_label = np.zeros(feature1_and_label_counts.shape)\n",
    "\n",
    "# P(feature|label) = P(feature, label) / P(label) =~ count(feature, label) / count(label).\n",
    "# Note that we could do this normalization more efficiently with array operations, but for the sake of clarity,\n",
    "# let's iterate over each label and each feature.\n",
    "for label in range(feature0_and_label_counts.shape[1]):\n",
    "    for feature in range(feature0_and_label_counts.shape[0]):\n",
    "        feature0_given_label[feature,label] = feature0_and_label_counts[feature,label] / label_counts[label]\n",
    "        feature1_given_label[feature,label] = feature1_and_label_counts[feature,label] / label_counts[label]\n",
    "\n",
    "# Here's our estimated conditional probability table.\n",
    "print 'Estimated values of P(feature=0|label):\\n', feature0_given_label\n",
    "\n",
    "# As a sanity check, which probabilities should sum to 1?\n",
    "print '\\nCheck that P(feature=0|label) + P(feature=1|label) = 1\\n',feature0_given_label + feature1_given_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pieces, let's try making a prediction for the first test example. It looks like this is a setosa (label 0) example with all small measurements -- all the feature values are 0.\n",
    "\n",
    "We start by assuming the prior distribution, which has a slight preference for virginica, followed by versicolor. Of course, these estimates come from our training data, which might not be a representative sample. In practice, we may prefer to use a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector: [ 0.  0.  0.  0.]\n",
      "Label: 0\n",
      "Prior: [0.31, 0.33, 0.36]\n"
     ]
    }
   ],
   "source": [
    "# What does the feature vector look like? And what's the true label?\n",
    "index = 0\n",
    "print 'Feature vector:', binarized_test_data[index]\n",
    "print 'Label:', test_labels[index]\n",
    "\n",
    "# Start with the prior distribution over labels.\n",
    "predictions = label_probs[:]\n",
    "print 'Prior:', predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of each feature as an additional piece of evidence. After observing the first feature, we update our belief by multiplying our initial probabilities by the probability of the observation, conditional on each possible label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After observing sepal length: [ 0.53448276  0.34482759  0.12068966]\n"
     ]
    }
   ],
   "source": [
    "# Let's include the first feature. We use feature0_given_label since the feature value is 0.\n",
    "predictions *= feature0_given_label[0]\n",
    "\n",
    "# We could wait until we've multiplied by all the feature probabilities, but there's no harm in normalizing after each update.\n",
    "predictions /= predictions.sum()\n",
    "print 'After observing sepal length:', predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after observing a short sepal, our updated belief prefers setosa. Let's include the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After observing all features: [ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Include the second feature.\n",
    "predictions *= feature0_given_label[1]\n",
    "predictions *= feature0_given_label[2]\n",
    "predictions *= feature0_given_label[3]\n",
    "\n",
    "# We could wait until we've multiplied by all the feature probabilities, but there's no harm in normalizing after each update.\n",
    "predictions /= predictions.sum()\n",
    "print 'After observing all features:', predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?\n",
    "\n",
    "Well, it looks like Naive Bayes came up with the right answer. But it seems overconfident!\n",
    "\n",
    "Let's look again at our conditional probability estimates for the features. Notice that there are a bunch of zero probabilities. This is bad because as soon as we multiply anything by zero, we're guaranteed that our final estimate will be zero. This is an overly harsh penalty for an observation that simply never occurred in our training data. Surely there's some possibility, even if very small, that there could exist a setosa with a long sepal.\n",
    "\n",
    "This is where smoothing comes in. The maximum likelihood estimate is only optimal in the case where we have infinite training data. When we have less than that, we need to temper maximum likelihood by reserving some small probability for unseen events. The simplest way to do this is with Laplace smoothing -- rather than starting with a count of 0 for each joint (feature, label) observation, we start with a count of $\\alpha$.\n",
    "\n",
    "Let's package training and inference into a class, modeled after sklearn's BernoulliNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    # Initialize an instance of the class.\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha     # additive (Laplace) smoothing parameter\n",
    "        self.priors = None     # estimated by fit()\n",
    "        self.probs = None      # estimated by fit()\n",
    "        self.num_labels = 0    # set by fit()\n",
    "        self.num_features = 0  # set by fit()\n",
    "        \n",
    "    def fit(self, train_data, train_labels):\n",
    "        # Store number of labels, number of features, and number training examples.\n",
    "        self.num_labels = len(np.unique(train_labels))\n",
    "        self.num_features = train_data.shape[1]\n",
    "        self.num_examples = train_data.shape[0]\n",
    "        \n",
    "        # Initialize an array of label counts. Each label gets a smoothed count of 2*alpha because\n",
    "        # each feature value (0 and 1) gets an extra count of alpha.\n",
    "        label_counts = np.ones(self.num_labels) * self.alpha * 2\n",
    "\n",
    "        # Initialize an array of (feature=1, label) counts to alpha.\n",
    "        feature0_and_label_counts = np.ones([self.num_features, self.num_labels]) * self.alpha\n",
    "        \n",
    "        # Count features with value == 1.\n",
    "        for i in range(self.num_examples):\n",
    "            label = train_labels[i]\n",
    "            label_counts[label] += 1\n",
    "            for feature_index, feature_value in enumerate(train_data[i]):\n",
    "                feature0_and_label_counts[feature_index][label] += (feature_value == 1)\n",
    "\n",
    "        # Normalize to get probabilities P(feature=1|label).\n",
    "        self.probs = feature0_and_label_counts / label_counts\n",
    "        \n",
    "        # Normalize label counts to get prior probabilities P(label).\n",
    "        self.priors = label_counts / label_counts.sum()\n",
    "\n",
    "    # Make predictions for each test example and return results.\n",
    "    def predict(self, test_data):\n",
    "        results = []\n",
    "        for item in test_data:\n",
    "            results.append(self._predict_item(item))\n",
    "        return np.array(results)\n",
    "    \n",
    "    # Private function for making a single prediction.\n",
    "    def _predict_item(self, item):\n",
    "        # Make a copy of the prior probabilities.\n",
    "        predictions = self.priors.copy()\n",
    "        \n",
    "        # Multiply by each conditional feature probability.\n",
    "        for (index, value) in enumerate(item):\n",
    "            feature_probs = self.probs[index]\n",
    "            if not value: feature_probs = 1 - feature_probs\n",
    "            predictions *= feature_probs\n",
    "\n",
    "        # Normalize and return the label that gives the largest probability.\n",
    "        predictions /= predictions.sum()\n",
    "        return predictions.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare our implementation with the sklearn implementation. Do the predictions agree? What about the estimated parameters? Try changing alpha from 0 to 1.\n",
    "\n",
    "Note: I think there might be a bug in the sklearn code. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With alpha = 0.00\n",
      "[OUR implementation] total:  50  correct:  41  accuracy: 0.82\n",
      "sklearn accuracy: 0.52\n",
      "\n",
      "Our feature probabilities\n",
      "[[ 0.          0.39393939  0.80555556]\n",
      " [ 0.80645161  0.18181818  0.30555556]\n",
      " [ 0.          1.          1.        ]\n",
      " [ 0.          0.90909091  1.        ]]\n",
      "\n",
      "sklearn feature probabilities\n",
      "[[ 0.          0.39393939  0.80555556]\n",
      " [ 0.80645161  0.18181818  0.30555556]\n",
      " [ 0.          1.          1.        ]\n",
      " [ 0.          0.90909091  1.        ]]\n",
      "\n",
      "Our prior probabilities\n",
      "[ 0.31  0.33  0.36]\n",
      "\n",
      "sklearn prior probabilities\n",
      "[ 0.31  0.33  0.36]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0\n",
    "nb = NaiveBayes(alpha=alpha)\n",
    "nb.fit(binarized_train_data, train_labels)\n",
    "\n",
    "# Compute accuracy on the test data.\n",
    "preds = nb.predict(binarized_test_data)\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, test_labels):\n",
    "    if pred == label: correct += 1\n",
    "    total += 1\n",
    "print 'With alpha = %.2f' %alpha\n",
    "print '[OUR implementation] total: %3d  correct: %3d  accuracy: %3.2f' %(total, correct, 1.0*correct/total)\n",
    "\n",
    "# Compare to sklearn's implementation.\n",
    "clf = BernoulliNB(alpha=alpha)\n",
    "clf.fit(binarized_train_data, train_labels)\n",
    "print 'sklearn accuracy: %3.2f' %clf.score(binarized_test_data, test_labels)\n",
    "\n",
    "print '\\nOur feature probabilities\\n', nb.probs\n",
    "print '\\nsklearn feature probabilities\\n', np.exp(clf.feature_log_prob_).T\n",
    "\n",
    "print '\\nOur prior probabilities\\n', nb.priors\n",
    "print '\\nsklearn prior probabilities\\n', np.exp(clf.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
